#
# Extract list of Astounding Science Fiction stories
# --------------------------------------------------
# from the site created by Andrew May:
# https://www.andrew-may.com/asf/list.htm
#
# This code was generated by ChatGPT 5 and modified by me.
# August/September 2025
#

import ssl
import certifi
import urllib.request
from bs4 import BeautifulSoup
import pandas as pd
import re

# --------------------------------------------------
#
# function normalize_author 
#
def normalize_author(name: str) -> str:
    """Convert to 'Last, First [Suffix]' form."""
    if not isinstance(name, str):
        return name
    name = name.strip()
    if not name:
        return name

    # Recognize suffixes like Jr., Sr., II, III
    suffix_re = re.compile(r'(?:,\s*|\s+)(Jr\.?|Sr\.?|II|III|IV|V)$', re.IGNORECASE)
    m = suffix_re.search(name)
    suffix = ""
    if m:
        suffix = m.group(1).strip()
        name = name[:m.start()].strip().rstrip(',')

    # If already "Last, First" keep it
    if ',' in name:
        last, rest = [p.strip() for p in name.split(',', 1)]
        result = f"{last}, {rest}" if rest else last
    else:
        tokens = name.split()
        if len(tokens) == 1:
            result = tokens[0]
        else:
            particles = {
                "van", "von", "de", "del", "di", "da", "la", "le", "du",
                "dos", "st", "st.", "ter", "van der", "van den", "de la"
            }
            two_word = ""
            if len(tokens) >= 3:
                last_two = (tokens[-2] + " " + tokens[-1]).lower()
                if last_two in particles:
                    two_word = last_two
            if two_word:
                last = tokens[-2] + " " + tokens[-1]
                first = " ".join(tokens[:-2])
            elif tokens[-2].lower() in particles:
                last = tokens[-2] + " " + tokens[-1]
                first = " ".join(tokens[:-2])
            else:
                last = tokens[-1]
                first = " ".join(tokens[:-1])
            result = f"{last}, {first}" if first else last

    if suffix:
        result = f"{result} {suffix}"

    result = " ".join(result.split())

    result = change_aliases(result)

    return result

# Define aliases (and correct multiple/mis-spellings)
# List of aliases is in square brackets, those are replaced
# with the author name before the bracketed list.
alias_map = {
    "Heinlein, Robert A.": ["Heinlein, Robert A.,", "MacDonald, Anson", "Saunders, Caleb"],
    "Hubbard, L. Ron": ["Hubbard, L. Ron", "La Fayette, Rene", "Lafayette, Ren√©"],
    "H Kuttner & CL Moore": ["H Kuttner & CL Moore", "Padgett, Lewis", "O'Donnell, Lawrence"],
    "Cole, Everett B.": ["Cole, E. B."],
    "Davis, Chandler": ["Davis, Chan"],
    "Edwards, Dolton": ["Edwards, D. M."],
    "Fyfe, H. B. ": ["Fyfe, Horace B."],
    "Gunn, James E.": ["Gunn, James"],
    "Ing, Dean C.": ["Ing, Dean"],
    "Cox, Irving E. Jr.": ["Cox, Irving Jr."],
    "Long, Frank Belknap": ["Long, Frank B."],
    "Marks, Winston K.": ["Marks, Winston"],
    "Saari, Oliver E.": ["Saari, Oliver"],
    "Smith, E. E.": ["Smith, Edward E."],
    "Stopa, John": ["Stopa, Jon"],
    "van Vogt, A. E.": ["Van Vogt, A.E."],
    "Alfred Coppel, Jr.": ["Alfred Coppel"],
    "Leinster, Murray": ["Jenkins, Will F."],
    "Kornbluth, C. M. & Merril, Judith": ["Judd, Cyril"]
}

# --------------------------------------------------
def change_aliases(author):
    for main_name, aliases in alias_map.items():
        if any(alias.lower() in str(author).lower() for alias in aliases):
            return main_name
    return author  # if no match, keep original


# -------------------------------------------------------------------------------------------
# Avoid issues with web site certificates and pretend that we are a Chrome browser.
context = ssl.create_default_context(cafile=certifi.where())
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

# -------------------------------------------------------------------------------------------
# Try requesting the page from the server. If that fails, display an error message and exit.
url = 'https://www.andrew-may.com/asf/list.htm'
req = urllib.request.Request(url, headers=headers)

try:
    with urllib.request.urlopen(req, context=context) as response:
        html = response.read()

except urllib.error.HTTPError as e:
    print(f"HTTP Error: {e.code} - {e.reason}")
    raise SystemExit(1)

# -------------------------------------------------------------------------------------------
# Parse the html.
soup = BeautifulSoup(html, "html.parser")

# -------------------------------------------------------------------------------------------
# GPT decided to just turn the lines of the html into a list of text strings,
# so we'll go with that approach.
for br in soup.find_all("br"):
    br.replace_with("\n")

text = soup.get_text()
lines = text.splitlines()

# records will eventually be converted to a pandas dataframe object,
# which will then be written out as a CSV file.
records = []
current_issue = None

months = [
    "January","February","March","April","May","June",
    "July","August","September","October","November","December"
]

#--------------------------------------------------------
# Start: process each line.

for line in lines:
    line = line.strip()

    # If this line is empty, skip it.
    if not line:
        continue

    # Try to match <monthName>space<fourDigits>
    is_issue_line = False;
    words = line.split()
    if len(words) == 2:
        if (words[0] in months) & (re.match(r"\d\d\d\d",words[1]) != None):
            is_issue_line = True
            current_issue = line

    # If this line does not include a month name, process the line.
    if not is_issue_line:

        # Try to match "Title (Author)"
        m = re.match(r"^(.*)\(([^()]*)\)$", line)

        if m:
            title = m.group(1).strip()
            author = m.group(2).strip()
        else:
            continue

        # Split issue into month + year
        if current_issue:
            parts = current_issue.split()
            if len(parts) == 2:
                month, year = parts
            else:
                month, year = current_issue, ""
        else:
            month, year = "", ""

        records.append({
            "Year": year,
            "Month": month,
            "Title": title,
            "Published_As": author
        })

# End: process each line.
#--------------------------------------------------------

df = pd.DataFrame(records)

# Create a new column 'Normalized_Author'
df["Author"] = df["Published_As"].apply(normalize_author)

df.to_csv("astounding_contents.csv")
print("CSV file created: astounding_contents.csv")

num_rows = len(df)
print(f"Number of stories: {num_rows} (each serial installment=1 story)")

# Count unique values in the normalized (de-aliased) author column
author_count = df['Author'].nunique()
print(f"Number of authors: {author_count}")

print(' ')

